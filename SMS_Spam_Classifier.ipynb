{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemant-1Kumar/SMS-Spam-Classifier/blob/main/SMS_Spam_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  SMS Spam Classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pervasive issue in SMS communication. These spam messages not only cause inconvenience but also pose potential security risks to users. As a result, the development of effective spam detection systems is critical. In this project, I have focused on building a machine learning model to classify SMS messages as either spam or legitimate (ham), using the SMS Spam Collection dataset as the foundation.\n",
        "\n",
        "The SMS Spam Collection dataset comprises 5,574 SMS messages, each labeled either as ham or spam. The dataset is structured into two columns: the first column contains the label (ham or spam), and the second contains the raw text of the message. The messages in the dataset have been collected from multiple sources. A significant portion of spam messages, around 425, was manually extracted from the Grumbletext website, which serves as a UK-based forum where users report spam. Another key contributor is the NUS SMS Corpus, from which 3,375 legitimate (ham) messages were randomly selected. These messages primarily originate from Singaporean university students. Additionally, 450 legitimate SMS messages were sourced from Caroline Tag's PhD thesis, and the SMS Spam Corpus v.0.1 Big added 1,002 legitimate messages and 322 spam messages to the dataset.\n",
        "\n",
        "The primary aim of this project is to develop an efficient machine learning model capable of accurately distinguishing between spam and ham messages. The first step involves data preprocessing, where the raw text of the SMS messages is cleaned and prepared for analysis. Techniques such as tokenization, removal of stop words, and normalization are applied to standardize the text data. This process is crucial for ensuring that the machine learning algorithms can effectively learn patterns in the data.\n",
        "\n",
        "Following data preparation, several machine learning models, including Logistic Regression, Naive Bayes, and Support Vector Machines (SVM), are trained on the dataset. The performance of these models is evaluated using standard metrics such as accuracy, precision, recall, and F1-score to determine the best-performing algorithm. The final model provides a reliable mechanism for identifying and filtering out spam messages, ultimately enhancing user experience and security in SMS communication.\n",
        "\n",
        "This project highlights the power of machine learning in addressing real-world challenges like spam detection. The insights and results from this work can contribute to the development of more advanced spam filtering systems in the future.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Hemant-1Kumar/SMS-Spam-Classifier"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the increasing use of SMS communication, the prevalence of unsolicited messages, commonly referred to as spam, has risen significantly. Spam messages not only disrupt user experience but can also carry potential security threats, such as phishing attacks and fraud. Manually filtering out these spam messages is both inefficient and impractical, given the sheer volume of messages exchanged daily.\n",
        "\n",
        "There is a critical need for an automated system that can accurately and efficiently distinguish between legitimate SMS messages (ham) and spam messages. This requires the application of machine learning techniques to analyze textual data and classify messages accordingly. The challenge lies in building a model that can generalize well to unseen data, minimizing both false positives (misclassifying ham as spam) and false negatives (failing to detect spam).\n",
        "\n",
        "This project seeks to address the problem by developing a robust machine learning model, trained on a labeled dataset of SMS messages, to accurately classify incoming messages as either spam or ham. The goal is to enhance user security and improve the overall efficiency of SMS communication systems by reducing the impact of spam messages."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Import Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "import nltk\n",
        "!pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking number of rows and columns of the dataset using shape\n",
        "print(\"Number of rows are: \",df.shape[0])\n",
        "print(\"Number of columns are: \",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=False)\n",
        ""
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check Unique Values for each variable using a for loop\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in\",i,\"is\",df[i].nunique())\n",
        ""
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning**"
      ],
      "metadata": {
        "id": "jYrylaB7aeMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "FwJSTx2TayCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#Drop last 3 columns\n",
        "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "id": "hxpny9BzbWTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming the columns\n",
        "df.rename(columns={'v1':'target','v2':'text'},inplace=True)\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "7eI-Bc8sbmcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "df['target'] = encoder.fit_transform(df['target'])\n"
      ],
      "metadata": {
        "id": "6w0Mv7GldDF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "x2sQTSnjf1X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Y0kzrcqwf--E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicate values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "vP1KKyeKgKin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicates\n",
        "df = df.drop_duplicates(keep='first')\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "Ks_yd_B5gO54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "gM_cd0IHgSUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "WUob-60zgbzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].value_counts()"
      ],
      "metadata": {
        "id": "bjd9aGOJgfx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data is imbalanced**"
      ],
      "metadata": {
        "id": "Ec6xiqX6gqfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_characters'] = df['text'].apply(len)"
      ],
      "metadata": {
        "id": "xz24iojIhDuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Wr2G8bOvhDrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num of words\n",
        "df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))"
      ],
      "metadata": {
        "id": "wW7z_xo2hDlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "TuBeGOGVhDii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))"
      ],
      "metadata": {
        "id": "V3T5a_OFhDe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "RpZjofcjhDbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['num_characters','num_words','num_sentences']].describe()"
      ],
      "metadata": {
        "id": "tXVwnlahhDXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ham\n",
        "df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()"
      ],
      "metadata": {
        "id": "kQsY7dxVhDTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spam\n",
        "df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()"
      ],
      "metadata": {
        "id": "rLpf9iXihDK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(df[df['target'] == 0]['num_characters'])\n",
        "sns.histplot(df[df['target'] == 1]['num_characters'],color='red')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "sns.pairplot(df,hue='target')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Create heatmap with the correlation of numeric columns\n",
        "sns.heatmap(numeric_df.corr(), annot=True)\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lower case\n",
        "* Tokenization\n",
        "* Removing special characters\n",
        "* Removing stop words and punctuation\n",
        "*Stemming\n",
        "\n"
      ],
      "metadata": {
        "id": "lPZYFWHHRZJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_text(text):\n",
        "    text = text.lower()\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    y = []\n",
        "    for i in text:\n",
        "        if i.isalnum():\n",
        "            y.append(i)\n",
        "\n",
        "    text = y[:]\n",
        "    y.clear()\n",
        "\n",
        "    for i in text:\n",
        "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "            y.append(i)\n",
        "\n",
        "    text = y[:]\n",
        "    y.clear()\n",
        "\n",
        "    for i in text:\n",
        "        y.append(ps.stem(i))\n",
        "\n",
        "\n",
        "    return \" \".join(y)"
      ],
      "metadata": {
        "id": "sY0k31ARRq8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_text(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")\n"
      ],
      "metadata": {
        "id": "dFXsG3-y2bl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][10]"
      ],
      "metadata": {
        "id": "A4wEpRcS2caD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "ps.stem('loving')"
      ],
      "metadata": {
        "id": "QnhkzWdw2cW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['transformed_text'] = df['text'].apply(transform_text)"
      ],
      "metadata": {
        "id": "ILJ8D0EK2cTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5OJpwSEy3YoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')"
      ],
      "metadata": {
        "id": "4CWRX6c23YlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \"))\n"
      ],
      "metadata": {
        "id": "d4s_us-b3Ydl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "plt.imshow(spam_wc)\n"
      ],
      "metadata": {
        "id": "UgTOxITO3YZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))\n"
      ],
      "metadata": {
        "id": "y5PmmzOY3j9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.imshow(ham_wc)"
      ],
      "metadata": {
        "id": "RN93BNJj3j6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "XMt8WLO83j3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_corpus = []\n",
        "for msg in df[df['target'] == 1]['transformed_text'].tolist():\n",
        "    for word in msg.split():\n",
        "        spam_corpus.append(word)\n",
        ""
      ],
      "metadata": {
        "id": "3pewQX213j0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(spam_corpus)"
      ],
      "metadata": {
        "id": "kIWYbNcN31VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "most_common_words = Counter(spam_corpus).most_common(30)\n",
        "\n",
        "df_common_words = pd.DataFrame(most_common_words, columns=['word', 'count'])\n",
        "\n",
        "sns.barplot(x='word', y='count', data=df_common_words)\n",
        "\n",
        "plt.xticks(rotation='vertical')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MyFFv4qC31SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_corpus = []\n",
        "for msg in df[df['target'] == 0]['transformed_text'].tolist():\n",
        "    for word in msg.split():\n",
        "        ham_corpus.append(word)"
      ],
      "metadata": {
        "id": "lGp9H-BJ31PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ham_corpus)"
      ],
      "metadata": {
        "id": "iwHRmhDO31L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Vectorization\n",
        "# using Bag of Words\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qXHbSshP31Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "cv = CountVectorizer()\n",
        "tfidf = TfidfVectorizer(max_features=3000)"
      ],
      "metadata": {
        "id": "UP8tfeSM6a_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tfidf.fit_transform(df['transformed_text']).toarray()\n"
      ],
      "metadata": {
        "id": "qJvgAFad8f46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "syCmUs6Z8f12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['target'].values"
      ],
      "metadata": {
        "id": "t2SI4f7x8fyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "SAIMVZ4WcpWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "Fr9jrgZw8wbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n"
      ],
      "metadata": {
        "id": "bdCaHr3d8wX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score"
      ],
      "metadata": {
        "id": "9SzUVnWc8xav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()"
      ],
      "metadata": {
        "id": "_ybk-WJK8xXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb.fit(X_train,y_train)\n",
        "y_pred1 = gnb.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred1))\n",
        "print(confusion_matrix(y_test,y_pred1))\n",
        "print(precision_score(y_test,y_pred1))"
      ],
      "metadata": {
        "id": "2rXIrbZQ8xUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnb.fit(X_train,y_train)\n",
        "y_pred2 = mnb.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred2))\n",
        "print(confusion_matrix(y_test,y_pred2))\n",
        "print(precision_score(y_test,y_pred2))"
      ],
      "metadata": {
        "id": "JjGGnTl98xRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb.fit(X_train,y_train)\n",
        "y_pred3 = bnb.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred3))\n",
        "print(confusion_matrix(y_test,y_pred3))\n",
        "print(precision_score(y_test,y_pred3))"
      ],
      "metadata": {
        "id": "CnbwBlCN8wUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
        "knc = KNeighborsClassifier()\n",
        "mnb = MultinomialNB()\n",
        "dtc = DecisionTreeClassifier(max_depth=5)\n",
        "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
        "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
        "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
        "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
        "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
        "xgb = XGBClassifier(n_estimators=50,random_state=2)"
      ],
      "metadata": {
        "id": "65xEqkId8m3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = {\n",
        "    'SVC' : svc,\n",
        "    'KN' : knc,\n",
        "    'NB': mnb,\n",
        "    'DT': dtc,\n",
        "    'LR': lrc,\n",
        "    'RF': rfc,\n",
        "    'AdaBoost': abc,\n",
        "    'BgC': bc,\n",
        "    'ETC': etc,\n",
        "    'GBDT':gbdt,\n",
        "    'xgb':xgb\n",
        "}"
      ],
      "metadata": {
        "id": "_hf691xy8m0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test,y_pred)\n",
        "    precision = precision_score(y_test,y_pred)\n",
        "\n",
        "    return accuracy,precision"
      ],
      "metadata": {
        "id": "3YwPVPqG8mxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classifier(svc,X_train,y_train,X_test,y_test)\n"
      ],
      "metadata": {
        "id": "TPRbThaR8mtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "\n",
        "for name,clf in clfs.items():\n",
        "\n",
        "    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)\n",
        "\n",
        "    print(\"For \",name)\n",
        "    print(\"Accuracy - \",current_accuracy)\n",
        "    print(\"Precision - \",current_precision)\n",
        "\n",
        "    accuracy_scores.append(current_accuracy)\n",
        "    precision_scores.append(current_precision)"
      ],
      "metadata": {
        "id": "TMqg0ls69tIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)\n"
      ],
      "metadata": {
        "id": "3Cw9_Tx19tDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df"
      ],
      "metadata": {
        "id": "OeQtIc3R9xrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df1 = pd.melt(performance_df, id_vars = \"Algorithm\")"
      ],
      "metadata": {
        "id": "bvVRofrh95QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df1"
      ],
      "metadata": {
        "id": "27yiq-7395Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.catplot(x='Algorithm', y='value',\n",
        "            hue='variable', data=performance_df1,\n",
        "            kind='bar', height=5)\n",
        "\n",
        "plt.ylim(0.5, 1.0)\n",
        "\n",
        "plt.xticks(rotation='vertical')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p36Y0MbG95Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model improve**\n",
        "1. Change the max_features parameter of TfIdf"
      ],
      "metadata": {
        "id": "c54lUZ88AR58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Voting Classifier\n",
        "svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)\n",
        "mnb = MultinomialNB()\n",
        "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "D5nDya7mAraR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')\n"
      ],
      "metadata": {
        "id": "dxvSaC2WA4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "phu6BES1A4c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = voting.predict(X_test)\n",
        "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "print(\"Precision\",precision_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "mrdDNIsgA4Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying stacking\n",
        "estimators=[('svm', svc), ('nb', mnb), ('et', etc)]\n",
        "final_estimator=RandomForestClassifier()"
      ],
      "metadata": {
        "id": "5OdVTXc7A4We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n"
      ],
      "metadata": {
        "id": "ZRrlxWBWA4TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n"
      ],
      "metadata": {
        "id": "1xP6XsIuA4Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "print(\"Precision\",precision_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "a8G29k3TA4MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
        "pickle.dump(mnb,open('model.pkl','wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}